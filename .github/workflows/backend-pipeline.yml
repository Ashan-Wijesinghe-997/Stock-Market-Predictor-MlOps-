# .github/workflows/backend-pipeline.yml
name: Backend CI/CD Pipeline

on:
  push:
    branches: [main, master]
    paths:
      - 'Stock-predictions-backend/**'
      - '.github/workflows/backend-pipeline.yml'
  pull_request:
    branches: [main, master]
    paths:
      - 'Stock-predictions-backend/**'
  workflow_dispatch:  # Manual trigger

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_DB: Stock_Predictor
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: Tha12345
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install backend dependencies
        run: |
          cd Stock-predictions-backend
          pip install -r requirements.txt

      - name: Set up test directories
        run: |
          mkdir -p Stock-predictions-backend/model_registry
          mkdir -p Stock-predictions-backend/monitoring_logs
          mkdir -p Stock-predictions-backend/monitoring_logs/predictions
          mkdir -p Stock-predictions-backend/monitoring_logs/performance
          mkdir -p Stock-predictions-backend/backtest_results
          mkdir -p Stock-predictions-backend/ml/tests
          touch Stock-predictions-backend/ml/tests/__init__.py

      - name: Create test for model performance
        run: |
          cd Stock-predictions-backend
          cat > ml/tests/test_model_performance.py << 'EOF'
          from django.test import TestCase
          from ml.model_registry import ModelRegistry
          from ml.backtesting import ModelBacktester
          import numpy as np
          import os
          import logging

          class ModelPerformanceTest(TestCase):
              def setUp(self):
                  # Set up directories
                  os.makedirs("model_registry", exist_ok=True)
                  os.makedirs("monitoring_logs", exist_ok=True)
                  os.makedirs("backtest_results", exist_ok=True)
                  
                  # Set up logging to capture any issues
                  logging.basicConfig(
                      level=logging.INFO,
                      format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
                  )
                  self.logger = logging.getLogger("model_test")
              
              def test_model_training_and_evaluation(self):
                  """Test that we can train a model and it performs reasonably well"""
                  self.logger.info("Starting model training and evaluation test")
                  
                  # Import the call_command function
                  from django.core.management import call_command
                  
                  # Training with sample data and sufficient data points
                  self.logger.info("Training model with sample data")
                  call_command(
                      'train_models', 
                      stocks='AAPL', 
                      use_sample_data=True, 
                      epochs=1, 
                      days=150,
                      min_data_points=150,
                      batch_size=32
                  )
                  
                  # Check that a model was created
                  registry = ModelRegistry()
                  models = registry.list_models(stock='AAPL')
                  self.assertTrue(len(models) > 0, "No model was created")
                  
                  # Get the latest model
                  latest_model = models[-1]
                  self.logger.info(f"Created model: {latest_model['model_id']}")
                  
                  # Backtest with shorter test period and sample data
                  backtester = ModelBacktester()
                  self.logger.info("Running backtest with sample data")
                  report = backtester.backtest_stock(
                      'AAPL', 
                      test_period_days=15,  # Shorter test period
                      use_sample_data=True
                  )
                  
                  # Check results
                  self.assertIsNotNone(report, "Backtest failed to produce a report")
                  
                  if report:
                      self.logger.info(f"Backtest metrics: RMSE={report['metrics']['rmse']:.4f}, MAE={report['metrics']['mae']:.4f}")
                      # Check reasonable performance (adjust threshold as needed)
                      self.assertLess(report['metrics']['rmse'], 50, "RMSE is too high")
                      
                      # Verify the report contains expected fields
                      required_fields = ['stock', 'model_id', 'metrics', 'results_file', 'plot_file']
                      for field in required_fields:
                          self.assertIn(field, report, f"Report missing required field: {field}")
                      
                      # Verify the files exist
                      self.assertTrue(os.path.exists(report['results_file']), "Results CSV file not created")
                      self.assertTrue(os.path.exists(report['plot_file']), "Plot file not created")
                      
              def tearDown(self):
                  # Clean up test artifacts if needed
                  self.logger.info("Test complete, cleaning up test artifacts")
          EOF

      - name: Run basic Django tests
        env:
          DATABASE_NAME: Stock_Predictor
          DATABASE_USER: postgres
          DATABASE_PASSWORD: Tha12345
          DATABASE_HOST: localhost
          DATABASE_PORT: 5432
        run: |
          cd Stock-predictions-backend
          python manage.py test ml.tests --exclude-tag=model_performance

      - name: Run model performance test
        env:
          DATABASE_NAME: Stock_Predictor
          DATABASE_USER: postgres
          DATABASE_PASSWORD: Tha12345
          DATABASE_HOST: localhost
          DATABASE_PORT: 5432
          MODEL_EPOCHS: 1
          MODEL_BATCH_SIZE: 32
        run: |
          cd Stock-predictions-backend
          python manage.py test ml.tests.test_model_performance

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-artifacts
          path: |
            Stock-predictions-backend/model_registry
            Stock-predictions-backend/backtest_results
            Stock-predictions-backend/model_training.log
  
  build-and-push:
    needs: test
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      
      - name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      
      - name: Build and push backend
        uses: docker/build-push-action@v4
        with:
          context: ./Stock-predictions-backend
          push: true
          tags: ${{ secrets.DOCKER_USERNAME }}/stock-prediction-backend:latest,${{ secrets.DOCKER_USERNAME }}/stock-prediction-backend:${{ github.sha }}
      
      - name: Create deployment tag
        run: |
          echo "DEPLOYMENT_TAG=$(date +'%Y%m%d%H%M%S')" >> $GITHUB_ENV
      
      - name: Create deployment artifact
        run: |
          mkdir -p deployment
          echo "Backend image: ${{ secrets.DOCKER_USERNAME }}/stock-prediction-backend:${{ github.sha }}" > deployment/backend-image.txt
          echo "Deployment tag: ${{ env.DEPLOYMENT_TAG }}" >> deployment/backend-image.txt
      
      - name: Upload deployment artifact
        uses: actions/upload-artifact@v4
        with:
          name: backend-deployment-info
          path: deployment/